{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337bdd48",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "êµ­ì  ì…ë ¥: ë¯¸êµ­\n",
      "ëª©ì  ì…ë ¥: \n",
      "âŒ ì¼ì¹˜í•˜ëŠ” ëª©ì  ì—†ìŒ. ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.\n",
      "ëª©ì  ì…ë ¥: ê´€ê´‘\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'ê²¨ìš¸'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 7ï¸âƒ£ ì •ê·œí™”\u001b[39;00m\n\u001b[0;32m     55\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m---> 56\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# 8ï¸âƒ£ ë°ì´í„° ë¶„í• \u001b[39;00m\n\u001b[0;32m     59\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_scaled, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\base.py:859\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:427\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:466\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinMaxScaler does not support sparse input. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using MaxAbsScaler instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m     )\n\u001b[0;32m    465\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 466\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m data_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    474\u001b[0m data_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\pandas\\core\\generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'ê²¨ìš¸'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# âœ… í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "# 1ï¸âƒ£ CSV ë¡œë“œ\n",
    "df = pd.read_csv('../../jin/data/processed/ì™¸êµ­ì¸ì…êµ­ì_ì „ì²˜ë¦¬ì™„ë£Œ_ë”¥ëŸ¬ë‹ìš©.csv', encoding='utf-8')\n",
    "df = df.fillna(0)\n",
    "\n",
    "# 2ï¸âƒ£ ì‚¬ìš©ì ì…ë ¥ ê²€ì¦ í•¨ìˆ˜\n",
    "def get_valid_input(column_name):\n",
    "    unique_vals = df[column_name].unique()\n",
    "    while True:\n",
    "        user_input = input(f\"{column_name} ì…ë ¥: \").strip().lower().replace(' ', '')\n",
    "        match = None\n",
    "        for val in unique_vals:\n",
    "            val_clean = val.strip().lower().replace(' ', '')\n",
    "            if user_input == val_clean:\n",
    "                match = val\n",
    "                break\n",
    "        if match:\n",
    "            return match\n",
    "        else:\n",
    "            print(f\"âŒ ì¼ì¹˜í•˜ëŠ” {column_name} ì—†ìŒ. ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "\n",
    "# 3ï¸âƒ£ êµ­ê°€ + ëª©ì  ì…ë ¥\n",
    "country = get_valid_input('êµ­ì ')\n",
    "purpose = get_valid_input('ëª©ì ')\n",
    "\n",
    "# 4ï¸âƒ£ ì¡°ê±´ í•„í„°ë§\n",
    "df = df[(df['êµ­ì '] == country) & (df['ëª©ì '] == purpose)]\n",
    "\n",
    "# 5ï¸âƒ£ Feature ì—”ì§€ë‹ˆì–´ë§: Lag, ëˆ„ì , ë¶„ê¸°, ê³„ì ˆ\n",
    "df['lag_1'] = df['ì…êµ­ììˆ˜'].shift(1)\n",
    "df['lag_3'] = df['ì…êµ­ììˆ˜'].shift(3)\n",
    "df['lag_12'] = df['ì…êµ­ììˆ˜'].shift(12)\n",
    "df['ì…êµ­ììˆ˜_cumsum'] = df['ì…êµ­ììˆ˜'].cumsum()\n",
    "df = df.dropna()\n",
    "\n",
    "# 6ï¸âƒ£ Feature ì„ íƒ\n",
    "features = ['ì—°ë„', 'ì›”', 'ë¶„ê¸°', 'ê³„ì ˆ', 'lag_1', 'lag_3', 'lag_12', 'ì…êµ­ììˆ˜_cumsum']\n",
    "X = df[features]\n",
    "y = df['ì…êµ­ììˆ˜']\n",
    "\n",
    "# 7ï¸âƒ£ ì •ê·œí™”\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 8ï¸âƒ£ ë°ì´í„° ë¶„í• \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 9ï¸âƒ£ XGBoost + í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "params = {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5]}\n",
    "grid = GridSearchCV(XGBRegressor(), params, cv=2)\n",
    "grid.fit(X_train, y_train)\n",
    "xgb_model = grid.best_estimator_\n",
    "\n",
    "# ğŸ”Ÿ ì˜ˆì¸¡ + í‰ê°€\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "\n",
    "print(f\"âœ… XGBoost MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 1ï¸âƒ£1ï¸âƒ£ ì¤‘ìš”ë„ ê·¸ë˜í”„\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, xgb_model.feature_importances_)\n",
    "plt.title('XGBoost Feature ì¤‘ìš”ë„')\n",
    "plt.show()\n",
    "\n",
    "# 1ï¸âƒ£2ï¸âƒ£ Prophetìš© Longí˜• + íœ´ì¼\n",
    "prophet_df = df[['ì—°ì›”', 'ì…êµ­ììˆ˜']].copy()\n",
    "prophet_df.rename(columns={'ì—°ì›”': 'ds', 'ì…êµ­ììˆ˜': 'y'}, inplace=True)\n",
    "prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])\n",
    "\n",
    "m = Prophet()\n",
    "m.add_country_holidays(country_name='KR')  # í•œêµ­ íœ´ì¼\n",
    "\n",
    "m.fit(prophet_df)\n",
    "\n",
    "future = m.make_future_dataframe(periods=12, freq='M')\n",
    "forecast = m.predict(future)\n",
    "\n",
    "fig = m.plot(forecast)\n",
    "plt.title('Prophet ì˜ˆì¸¡ (í•œêµ­ íœ´ì¼ í¬í•¨)')\n",
    "plt.show()\n",
    "\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cda86abc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "êµ­ì  ì…ë ¥ (ì—†ìœ¼ë©´ Enter): ì˜êµ­\n",
      "ëª©ì  ì…ë ¥ (ì—†ìœ¼ë©´ Enter): \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'ê²¨ìš¸'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# 9ï¸âƒ£ ì •ê·œí™”\u001b[39;00m\n\u001b[0;32m     67\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m---> 68\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# ğŸ”Ÿ ë°ì´í„° ë¶„í• \u001b[39;00m\n\u001b[0;32m     71\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     72\u001b[0m     X_scaled, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     73\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\base.py:859\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:427\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:466\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinMaxScaler does not support sparse input. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using MaxAbsScaler instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m     )\n\u001b[0;32m    465\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 466\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m data_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    474\u001b[0m data_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\han\\lib\\site-packages\\pandas\\core\\generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'ê²¨ìš¸'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# âœ… í•œê¸€ í°íŠ¸ ê¹¨ì§ ë°©ì§€\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "# 1ï¸âƒ£ CSV ë¡œë“œ\n",
    "df = pd.read_csv('../../jin/data/processed/ì™¸êµ­ì¸ì…êµ­ì_ì „ì²˜ë¦¬ì™„ë£Œ_ë”¥ëŸ¬ë‹ìš©.csv', encoding='utf-8')\n",
    "df = df.fillna(0)\n",
    "\n",
    "# 2ï¸âƒ£ ì‚¬ìš©ì ì…ë ¥ ê²€ì¦ í•¨ìˆ˜\n",
    "def get_valid_input(column_name, allow_blank=False):\n",
    "    unique_vals = df[column_name].unique()\n",
    "    while True:\n",
    "        user_input = input(f\"{column_name} ì…ë ¥ (ì—†ìœ¼ë©´ Enter): \").strip().lower().replace(' ', '')\n",
    "        if allow_blank and user_input == \"\":\n",
    "            return None\n",
    "        match = None\n",
    "        for val in unique_vals:\n",
    "            val_clean = val.strip().lower().replace(' ', '')\n",
    "            if user_input == val_clean:\n",
    "                match = val\n",
    "                break\n",
    "        if match:\n",
    "            return match\n",
    "        else:\n",
    "            print(f\"âŒ ì¼ì¹˜í•˜ëŠ” {column_name} ì—†ìŒ. ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "\n",
    "# 3ï¸âƒ£ êµ­ê°€ ì…ë ¥ (í•„ìˆ˜)\n",
    "country = get_valid_input('êµ­ì ')\n",
    "\n",
    "# 4ï¸âƒ£ ëª©ì  ì…ë ¥ (ì„ íƒ)\n",
    "purpose = get_valid_input('ëª©ì ', allow_blank=True)\n",
    "\n",
    "# 5ï¸âƒ£ ì¡°ê±´ í•„í„°ë§ (Wide ê¸°ì¤€)\n",
    "df = df[df['êµ­ì '] == country]\n",
    "if purpose:\n",
    "    df = df[df['ëª©ì '] == purpose]\n",
    "\n",
    "if df.empty:\n",
    "    raise ValueError(\"âš ï¸ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# âœ… Wide â†’ Long (Prophet & ML ë™ì¼)\n",
    "date_cols = [col for col in df.columns if 'ë…„' in col and 'ì›”' in col]\n",
    "\n",
    "long_df = df.melt(\n",
    "    id_vars=['êµ­ì ', 'ëª©ì ', 'ì—°ë„', 'ì›”', 'ë¶„ê¸°', 'ê³„ì ˆ', 'ì½”ë¡œë‚˜ê¸°ê°„'],\n",
    "    value_vars=date_cols,\n",
    "    var_name='ì—°ì›”',\n",
    "    value_name='ì…êµ­ììˆ˜'\n",
    ")\n",
    "\n",
    "# âœ… ì—°ì›” ì²˜ë¦¬\n",
    "long_df['ì—°ì›”'] = long_df['ì—°ì›”'].str.replace('ë…„', '-').str.replace('ì›”', '')\n",
    "long_df['ds'] = pd.to_datetime(long_df['ì—°ì›”'] + '-01')\n",
    "long_df['ì…êµ­ììˆ˜'] = long_df['ì…êµ­ììˆ˜'].astype(str).str.replace(',', '').astype(float)\n",
    "\n",
    "# âœ… Feature Engineering\n",
    "long_df['lag_1'] = long_df['ì…êµ­ììˆ˜'].shift(1)\n",
    "long_df['lag_3'] = long_df['ì…êµ­ììˆ˜'].shift(3)\n",
    "long_df['lag_12'] = long_df['ì…êµ­ììˆ˜'].shift(12)\n",
    "long_df['ì…êµ­ììˆ˜_cumsum'] = long_df['ì…êµ­ììˆ˜'].cumsum()\n",
    "\n",
    "long_df = long_df.dropna()\n",
    "\n",
    "# âœ… Feature ì„ íƒ\n",
    "features = ['ì—°ë„', 'ì›”', 'ë¶„ê¸°', 'ê³„ì ˆ', 'ì½”ë¡œë‚˜ê¸°ê°„', 'lag_1', 'lag_3', 'lag_12', 'ì…êµ­ììˆ˜_cumsum']\n",
    "X = long_df[features]\n",
    "y = long_df['ì…êµ­ììˆ˜']\n",
    "\n",
    "# âœ… ì •ê·œí™”\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# âœ… ë°ì´í„° ë¶„í• \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# âœ… XGBoost + GridSearchCV\n",
    "params = {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5]}\n",
    "grid = GridSearchCV(XGBRegressor(), params, cv=2)\n",
    "grid.fit(X_train, y_train)\n",
    "xgb_model = grid.best_estimator_\n",
    "\n",
    "# âœ… ì˜ˆì¸¡ + í‰ê°€\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f\"âœ… XGBoost MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# âœ… ì¤‘ìš”ë„ ê·¸ë˜í”„\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, xgb_model.feature_importances_)\n",
    "plt.title('XGBoost Feature ì¤‘ìš”ë„')\n",
    "plt.show()\n",
    "\n",
    "# âœ… Prophet ëª¨ë¸\n",
    "prophet_df = long_df[['ds', 'ì…êµ­ììˆ˜']].copy()\n",
    "prophet_df.rename(columns={'ì…êµ­ììˆ˜': 'y'}, inplace=True)\n",
    "\n",
    "m = Prophet()\n",
    "m.add_country_holidays(country_name='KR')\n",
    "m.fit(prophet_df)\n",
    "\n",
    "future = m.make_future_dataframe(periods=12, freq='M')\n",
    "forecast = m.predict(future)\n",
    "\n",
    "# âœ… Prophet ì‹œê°í™”\n",
    "fig = m.plot(forecast)\n",
    "plt.title('Prophet ì˜ˆì¸¡ (í•œêµ­ íœ´ì¼ í¬í•¨)')\n",
    "plt.show()\n",
    "\n",
    "# âœ… Prophet ê²°ê³¼\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b785c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# âœ… í•œê¸€ í°íŠ¸ ê¹¨ì§ ë°©ì§€\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "# 1ï¸âƒ£ CSV ë¡œë“œ\n",
    "df = pd.read_csv('ì™¸êµ­ì¸ì…êµ­ì_ì „ì²˜ë¦¬ì™„ë£Œ_ë”¥ëŸ¬ë‹ìš©.csv', encoding='cp949')\n",
    "\n",
    "# 2ï¸âƒ£ ì—°ì›” ì»¬ëŸ¼ ì•ˆì „ í•„í„°ë§\n",
    "if 'ì—°ì›”' in df.columns:\n",
    "    df = df[df['ì—°ì›”'].astype(str).str.contains('ë…„|ì›”|\\\\-')]\n",
    "\n",
    "# 3ï¸âƒ£ ì‚¬ìš©ì ì…ë ¥ ê²€ì¦ í•¨ìˆ˜\n",
    "def get_valid_input(column_name, allow_blank=False):\n",
    "    unique_vals = df[column_name].unique()\n",
    "    while True:\n",
    "        user_input = input(f\"{column_name} ì…ë ¥ (ì—†ìœ¼ë©´ Enter): \").strip().lower().replace(' ', '')\n",
    "        if allow_blank and user_input == \"\":\n",
    "            return None\n",
    "        match = None\n",
    "        for val in unique_vals:\n",
    "            val_clean = val.strip().lower().replace(' ', '')\n",
    "            if user_input == val_clean:\n",
    "                match = val\n",
    "                break\n",
    "        if match:\n",
    "            return match\n",
    "        else:\n",
    "            print(f\"âŒ ì¼ì¹˜í•˜ëŠ” {column_name} ì—†ìŒ. ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "\n",
    "# 4ï¸âƒ£ êµ­ê°€ ì…ë ¥ (í•„ìˆ˜)\n",
    "country = get_valid_input('êµ­ì ')\n",
    "\n",
    "# 5ï¸âƒ£ ëª©ì  ì…ë ¥ (ì„ íƒ)\n",
    "purpose = get_valid_input('ëª©ì ', allow_blank=True)\n",
    "\n",
    "# 6ï¸âƒ£ ì¡°ê±´ í•„í„°ë§\n",
    "df = df[df['êµ­ì '] == country]\n",
    "if purpose:\n",
    "    df = df[df['ëª©ì '] == purpose]\n",
    "\n",
    "if df.empty:\n",
    "    raise ValueError(\"âš ï¸ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ ì¢…ë£Œ.\")\n",
    "\n",
    "# 7ï¸âƒ£ Feature Engineering\n",
    "df['lag_1'] = df['ì…êµ­ììˆ˜'].shift(1)\n",
    "df['lag_3'] = df['ì…êµ­ììˆ˜'].shift(3)\n",
    "df['lag_12'] = df['ì…êµ­ììˆ˜'].shift(12)\n",
    "df['ì…êµ­ììˆ˜_cumsum'] = df['ì…êµ­ììˆ˜'].cumsum()\n",
    "df = df.dropna()\n",
    "\n",
    "# 8ï¸âƒ£ Feature ì„ íƒ\n",
    "features = ['ì—°ë„', 'ì›”', 'ë¶„ê¸°', 'ê³„ì ˆ', 'ì½”ë¡œë‚˜ê¸°ê°„', 'lag_1', 'lag_3', 'lag_12', 'ì…êµ­ììˆ˜_cumsum']\n",
    "X = df[features]\n",
    "y = df['ì…êµ­ììˆ˜']\n",
    "\n",
    "# 9ï¸âƒ£ ì •ê·œí™”\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ğŸ”Ÿ ë°ì´í„° ë¶„í• \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 1ï¸âƒ£1ï¸âƒ£ XGBoost + GridSearchCV\n",
    "params = {'n_estimators': [100], 'learning_rate': [0.05], 'max_depth': [3]}\n",
    "grid = GridSearchCV(XGBRegressor(), params, cv=2)\n",
    "grid.fit(X_train, y_train)\n",
    "xgb_model = grid.best_estimator_\n",
    "\n",
    "# 1ï¸âƒ£2ï¸âƒ£ ì˜ˆì¸¡ + í‰ê°€\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f\"âœ… XGBoost MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 1ï¸âƒ£3ï¸âƒ£ Feature ì¤‘ìš”ë„\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, xgb_model.feature_importances_)\n",
    "plt.title('XGBoost Feature ì¤‘ìš”ë„')\n",
    "plt.show()\n",
    "\n",
    "# 1ï¸âƒ£4ï¸âƒ£ Prophet Long ë³€í™˜ + ì•ˆì „í•œ ì—°ì›” ë³€í™˜\n",
    "df['ì—°ì›”'] = df['ì—°ì›”'].astype(str).str.replace('ë…„', '-').str.replace('ì›”', '')\n",
    "df['ds'] = pd.to_datetime(df['ì—°ì›”'] + '-01', errors='coerce')\n",
    "df = df[df['ds'].notna()]  # NaT ì œê±°\n",
    "\n",
    "prophet_df = df[['ds', 'ì…êµ­ììˆ˜']].rename(columns={'ì…êµ­ììˆ˜': 'y'})\n",
    "\n",
    "m = Prophet()\n",
    "m.add_country_holidays(country_name='KR')\n",
    "m.fit(prophet_df)\n",
    "\n",
    "future = m.make_future_dataframe(periods=12, freq='M')\n",
    "forecast = m.predict(future)\n",
    "\n",
    "fig = m.plot(forecast)\n",
    "plt.title('Prophet ì˜ˆì¸¡ (í•œêµ­ íœ´ì¼ í¬í•¨)')\n",
    "plt.show()\n",
    "\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(12))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "han",
   "language": "python",
   "name": "han"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
