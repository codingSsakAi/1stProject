# -*- coding: utf-8 -*-
"""
ëª¨ë¸ í‰ê°€ê¸° í´ë˜ìŠ¤
Author: Jin
Created: 2025-01-15
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import logging
import sys
import os
from typing import Dict, List, Any, Optional, Tuple
import warnings
from datetime import datetime
import json

# í”„ë¡œì íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from config.config import *
from scripts.utils import *
from scripts.data_loader import DataLoader
from scripts.lstm_model import LSTMModel

# í•œê¸€ í°íŠ¸ ì„¤ì •
setup_plotting()

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)


class ModelEvaluator:
    """ëª¨ë¸ í‰ê°€ê¸° í´ë˜ìŠ¤"""

    def __init__(self, data_loader: DataLoader = None):
        """
        ì´ˆê¸°í™”

        Args:
            data_loader: ë°ì´í„° ë¡œë” ì¸ìŠ¤í„´ìŠ¤
        """
        self.data_loader = data_loader or DataLoader()
        self.models = {}
        self.evaluation_results = {}
        self.predictions = {}

        logger.info("ğŸ“Š ëª¨ë¸ í‰ê°€ê¸° ì´ˆê¸°í™”")

    def load_model(self, model_name: str, model_path: str, model_class):
        """
        ëª¨ë¸ ë¡œë“œ

        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            model_class: ëª¨ë¸ í´ë˜ìŠ¤
        """
        try:
            model_instance = model_class()
            model_instance.load_model(model_path)

            self.models[model_name] = model_instance
            logger.info(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise

    def evaluate_model(
        self,
        model_name: str,
        X_test: np.ndarray,
        y_test: np.ndarray,
        X_val: np.ndarray = None,
        y_val: np.ndarray = None,
        detailed: bool = True,
    ) -> Dict[str, Any]:
        """
        ëª¨ë¸ í‰ê°€

        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            X_val, y_val: ê²€ì¦ ë°ì´í„° (ì„ íƒì‚¬í•­)
            detailed: ìƒì„¸ í‰ê°€ ì—¬ë¶€

        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if model_name not in self.models:
            raise ValueError(f"ë¡œë“œë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")

        logger.info(f"ğŸ“Š {model_name} ëª¨ë¸ í‰ê°€ ì‹œì‘")

        model = self.models[model_name]

        # ì˜ˆì¸¡ ìˆ˜í–‰
        y_pred_test = model.predict(X_test)
        self.predictions[f"{model_name}_test"] = y_pred_test

        # ê¸°ë³¸ í‰ê°€ ì§€í‘œ ê³„ì‚°
        test_metrics = calculate_metrics(y_test, y_pred_test)

        result = {
            "model_name": model_name,
            "test_metrics": test_metrics,
            "test_predictions": y_pred_test,
            "test_actual": y_test,
        }

        # ê²€ì¦ ë°ì´í„° í‰ê°€ (ìˆì„ ê²½ìš°)
        if X_val is not None and y_val is not None:
            y_pred_val = model.predict(X_val)
            val_metrics = calculate_metrics(y_val, y_pred_val)
            result["val_metrics"] = val_metrics
            result["val_predictions"] = y_pred_val
            result["val_actual"] = y_val
            self.predictions[f"{model_name}_val"] = y_pred_val

        # ìƒì„¸ í‰ê°€
        if detailed:
            detailed_analysis = self.detailed_evaluation(y_test, y_pred_test, model_name)
            result["detailed_analysis"] = detailed_analysis

        self.evaluation_results[model_name] = result

        logger.info(f"âœ… {model_name} ëª¨ë¸ í‰ê°€ ì™„ë£Œ")
        print_metrics(test_metrics, f"{model_name} í…ŒìŠ¤íŠ¸ ì„±ëŠ¥")

        return result

    def detailed_evaluation(
        self, y_true: np.ndarray, y_pred: np.ndarray, model_name: str
    ) -> Dict[str, Any]:
        """
        ìƒì„¸ í‰ê°€ ë¶„ì„

        Args:
            y_true: ì‹¤ì œê°’
            y_pred: ì˜ˆì¸¡ê°’
            model_name: ëª¨ë¸ ì´ë¦„

        Returns:
            ìƒì„¸ ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"ğŸ” {model_name} ìƒì„¸ í‰ê°€ ë¶„ì„ ì¤‘...")

        # ì”ì°¨ ê³„ì‚°
        residuals = y_true - y_pred

        # í†µê³„ì  ë¶„ì„
        analysis = {
            "residuals": {
                "mean": np.mean(residuals),
                "std": np.std(residuals),
                "min": np.min(residuals),
                "max": np.max(residuals),
                "median": np.median(residuals),
                "q25": np.percentile(residuals, 25),
                "q75": np.percentile(residuals, 75),
            },
            "distribution": {
                "skewness": stats.skew(residuals),
                "kurtosis": stats.kurtosis(residuals),
                "normality_test": stats.jarque_bera(residuals),
            },
            "correlation": {
                "pearson": stats.pearsonr(y_true, y_pred)[0],
                "spearman": stats.spearmanr(y_true, y_pred)[0],
            },
        }

        # ì˜ˆì¸¡ êµ¬ê°„ë³„ ì„±ëŠ¥
        percentiles = [0, 25, 50, 75, 100]
        y_true_percentiles = np.percentile(y_true, percentiles)

        performance_by_range = {}
        for i in range(len(percentiles) - 1):
            mask = (y_true >= y_true_percentiles[i]) & (y_true < y_true_percentiles[i + 1])
            if np.any(mask):
                range_metrics = calculate_metrics(y_true[mask], y_pred[mask])
                performance_by_range[f"P{percentiles[i]}-P{percentiles[i+1]}"] = range_metrics

        analysis["performance_by_range"] = performance_by_range

        return analysis

    def plot_predictions(self, model_name: str, save_plots: bool = True):
        """
        ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”

        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            save_plots: ê·¸ë˜í”„ ì €ì¥ ì—¬ë¶€
        """
        if model_name not in self.evaluation_results:
            logger.warning(f"í‰ê°€ë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")
            return

        result = self.evaluation_results[model_name]
        y_true = result["test_actual"]
        y_pred = result["test_predictions"]

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. ì‹œê³„ì—´ ì˜ˆì¸¡ ë¹„êµ
        axes[0, 0].plot(y_true, label="ì‹¤ì œê°’", alpha=0.7)
        axes[0, 0].plot(y_pred, label="ì˜ˆì¸¡ê°’", alpha=0.7)
        axes[0, 0].set_title(f"{model_name} - ì‹œê³„ì—´ ì˜ˆì¸¡ ë¹„êµ")
        axes[0, 0].set_xlabel("ì‹œê°„")
        axes[0, 0].set_ylabel("ì…êµ­ììˆ˜")
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # 2. ì‚°ì ë„ (ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’)
        axes[0, 1].scatter(y_true, y_pred, alpha=0.6)
        axes[0, 1].plot(
            [y_true.min(), y_true.max()],
            [y_true.min(), y_true.max()],
            "r--",
            lw=2,
            label="Perfect Prediction",
        )
        axes[0, 1].set_title(f"{model_name} - ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’")
        axes[0, 1].set_xlabel("ì‹¤ì œê°’")
        axes[0, 1].set_ylabel("ì˜ˆì¸¡ê°’")
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # 3. ì”ì°¨ ë¶„í¬
        residuals = y_true - y_pred
        axes[1, 0].hist(residuals, bins=50, alpha=0.7, density=True)
        axes[1, 0].axvline(
            np.mean(residuals), color="red", linestyle="--", label=f"í‰ê· : {np.mean(residuals):.2f}"
        )
        axes[1, 0].set_title(f"{model_name} - ì”ì°¨ ë¶„í¬")
        axes[1, 0].set_xlabel("ì”ì°¨")
        axes[1, 0].set_ylabel("ë°€ë„")
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # 4. ì”ì°¨ vs ì˜ˆì¸¡ê°’
        axes[1, 1].scatter(y_pred, residuals, alpha=0.6)
        axes[1, 1].axhline(y=0, color="red", linestyle="--")
        axes[1, 1].set_title(f"{model_name} - ì”ì°¨ vs ì˜ˆì¸¡ê°’")
        axes[1, 1].set_xlabel("ì˜ˆì¸¡ê°’")
        axes[1, 1].set_ylabel("ì”ì°¨")
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()

        if save_plots:
            save_path = PLOTS_DIR / f"{model_name.lower()}_evaluation.png"
            plt.savefig(save_path, dpi=300, bbox_inches="tight")
            logger.info(f"ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ê·¸ë˜í”„ ì €ì¥: {save_path}")

        plt.show()

    def plot_residual_analysis(self, model_name: str, save_plots: bool = True):
        """
        ì”ì°¨ ë¶„ì„ ì‹œê°í™”

        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            save_plots: ê·¸ë˜í”„ ì €ì¥ ì—¬ë¶€
        """
        if model_name not in self.evaluation_results:
            logger.warning(f"í‰ê°€ë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")
            return

        result = self.evaluation_results[model_name]
        y_true = result["test_actual"]
        y_pred = result["test_predictions"]
        residuals = y_true - y_pred

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. ì”ì°¨ ì‹œê³„ì—´
        axes[0, 0].plot(residuals, alpha=0.7)
        axes[0, 0].axhline(y=0, color="red", linestyle="--")
        axes[0, 0].set_title(f"{model_name} - ì”ì°¨ ì‹œê³„ì—´")
        axes[0, 0].set_xlabel("ì‹œê°„")
        axes[0, 0].set_ylabel("ì”ì°¨")
        axes[0, 0].grid(True, alpha=0.3)

        # 2. QQ í”Œë¡¯
        stats.probplot(residuals, dist="norm", plot=axes[0, 1])
        axes[0, 1].set_title(f"{model_name} - ì”ì°¨ QQ Plot")
        axes[0, 1].grid(True, alpha=0.3)

        # 3. ì”ì°¨ ë°•ìŠ¤í”Œë¡¯
        axes[1, 0].boxplot(residuals)
        axes[1, 0].set_title(f"{model_name} - ì”ì°¨ ë°•ìŠ¤í”Œë¡¯")
        axes[1, 0].set_ylabel("ì”ì°¨")
        axes[1, 0].grid(True, alpha=0.3)

        # 4. ì”ì°¨ ìê¸°ìƒê´€
        from pandas.plotting import autocorrelation_plot

        autocorrelation_plot(pd.Series(residuals), ax=axes[1, 1])
        axes[1, 1].set_title(f"{model_name} - ì”ì°¨ ìê¸°ìƒê´€")
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()

        if save_plots:
            save_path = PLOTS_DIR / f"{model_name.lower()}_residual_analysis.png"
            plt.savefig(save_path, dpi=300, bbox_inches="tight")
            logger.info(f"ğŸ“Š ì”ì°¨ ë¶„ì„ ê·¸ë˜í”„ ì €ì¥: {save_path}")

        plt.show()

    def compare_models(self, model_names: List[str] = None, save_plots: bool = True):
        """
        ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

        Args:
            model_names: ë¹„êµí•  ëª¨ë¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            save_plots: ê·¸ë˜í”„ ì €ì¥ ì—¬ë¶€
        """
        if model_names is None:
            model_names = list(self.evaluation_results.keys())

        logger.info(f"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ: {model_names}")

        # ë¹„êµ ë°ì´í„° ì¤€ë¹„
        comparison_data = []
        for model_name in model_names:
            if model_name in self.evaluation_results:
                result = self.evaluation_results[model_name]
                metrics = result["test_metrics"]
                comparison_data.append(
                    {
                        "Model": model_name,
                        "MAE": metrics["mae"],
                        "RMSE": metrics["rmse"],
                        "MAPE": metrics["mape"],
                        "RÂ²": metrics["r2"],
                    }
                )

        comparison_df = pd.DataFrame(comparison_data)

        # ì‹œê°í™”
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        metrics_to_plot = ["MAE", "RMSE", "MAPE", "RÂ²"]

        for i, metric in enumerate(metrics_to_plot):
            ax = axes[i // 2, i % 2]
            bars = ax.bar(comparison_df["Model"], comparison_df[metric])
            ax.set_title(f"ëª¨ë¸ë³„ {metric} ë¹„êµ")
            ax.set_ylabel(metric)

            # ê°’ í‘œì‹œ
            for bar in bars:
                height = bar.get_height()
                ax.text(
                    bar.get_x() + bar.get_width() / 2.0,
                    height,
                    f"{height:.4f}",
                    ha="center",
                    va="bottom",
                )

            ax.grid(True, alpha=0.3)

        plt.tight_layout()

        if save_plots:
            save_path = PLOTS_DIR / "model_comparison.png"
            plt.savefig(save_path, dpi=300, bbox_inches="tight")
            logger.info(f"ğŸ“Š ëª¨ë¸ ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {save_path}")

        plt.show()

        # ë¹„êµ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
        print("=" * 80)
        print(comparison_df.to_string(index=False))
        print("=" * 80)

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        best_models = {}
        for metric in metrics_to_plot:
            if metric == "RÂ²":
                best_idx = comparison_df[metric].idxmax()
            else:
                best_idx = comparison_df[metric].idxmin()
            best_models[metric] = comparison_df.loc[best_idx, "Model"]

        print("\nğŸ† ì§€í‘œë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸:")
        for metric, model in best_models.items():
            print(f"  {metric}: {model}")

        return comparison_df

    def performance_by_category(self, model_name: str, category_data: Dict[str, Any]):
        """
        ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„

        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            category_data: ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° (ì˜ˆ: êµ­ê°€ë³„, ëª©ì ë³„)
        """
        if model_name not in self.evaluation_results:
            logger.warning(f"í‰ê°€ë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")
            return

        logger.info(f"ğŸ“Š {model_name} ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„")

        result = self.evaluation_results[model_name]
        y_true = result["test_actual"]
        y_pred = result["test_predictions"]

        category_performance = {}

        for category, indices in category_data.items():
            if len(indices) > 0:
                cat_y_true = y_true[indices]
                cat_y_pred = y_pred[indices]
                cat_metrics = calculate_metrics(cat_y_true, cat_y_pred)
                category_performance[category] = cat_metrics

        # ê²°ê³¼ ì €ì¥
        category_df = pd.DataFrame(category_performance).T
        category_df.to_csv(
            EVALUATION_DIR / f"{model_name.lower()}_category_performance.csv", encoding="utf-8-sig"
        )

        return category_performance

    def generate_evaluation_report(self, model_names: List[str] = None) -> str:
        """
        í‰ê°€ ë³´ê³ ì„œ ìƒì„±

        Args:
            model_names: ë³´ê³ ì„œì— í¬í•¨í•  ëª¨ë¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë³´ê³ ì„œ í…ìŠ¤íŠ¸
        """
        logger.info("ğŸ“‹ í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì¤‘...")

        if model_names is None:
            model_names = list(self.evaluation_results.keys())

        report = f"""
# ëª¨ë¸ í‰ê°€ ë³´ê³ ì„œ

## ğŸ“Š í‰ê°€ ê°œìš”
- í‰ê°€ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- í‰ê°€ ëª¨ë¸ ìˆ˜: {len(model_names)}
- í‰ê°€ ì§€í‘œ: MAE, RMSE, MAPE, RÂ²

## ğŸ“ˆ ëª¨ë¸ë³„ ì„±ëŠ¥ ê²°ê³¼
"""

        for model_name in model_names:
            if model_name in self.evaluation_results:
                result = self.evaluation_results[model_name]
                metrics = result["test_metrics"]

                report += f"""
### {model_name}
- **MAE**: {metrics['mae']:.4f}
- **RMSE**: {metrics['rmse']:.4f}
- **MAPE**: {metrics['mape']:.4f}%
- **RÂ²**: {metrics['r2']:.4f}

"""

                # ìƒì„¸ ë¶„ì„ (ìˆì„ ê²½ìš°)
                if "detailed_analysis" in result:
                    detailed = result["detailed_analysis"]
                    report += f"""
#### ìƒì„¸ ë¶„ì„
- **ì”ì°¨ í‰ê· **: {detailed['residuals']['mean']:.4f}
- **ì”ì°¨ í‘œì¤€í¸ì°¨**: {detailed['residuals']['std']:.4f}
- **ìƒê´€ê´€ê³„ (Pearson)**: {detailed['correlation']['pearson']:.4f}
- **ì •ê·œì„± ê²€ì • p-value**: {detailed['distribution']['normality_test'][1]:.4f}

"""

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ ì •
        best_model = None
        best_score = float("inf")

        for model_name in model_names:
            if model_name in self.evaluation_results:
                score = self.evaluation_results[model_name]["test_metrics"]["mape"]
                if score < best_score:
                    best_score = score
                    best_model = model_name

        report += f"""
## ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸
- **ëª¨ë¸ëª…**: {best_model}
- **MAPE ì ìˆ˜**: {best_score:.4f}%

## ğŸ“‹ ê¶Œì¥ì‚¬í•­
1. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì¸ {best_model} ì‚¬ìš© ê¶Œì¥
2. ì”ì°¨ ë¶„ì„ ê²°ê³¼ í™•ì¸ í•„ìš”
3. ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ì°¨ì´ ë¶„ì„ ìˆ˜í–‰
4. ì¶”ê°€ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê³ ë ¤

---
ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

        # ë³´ê³ ì„œ ì €ì¥
        with open(EVALUATION_DIR / "evaluation_report.md", "w", encoding="utf-8") as f:
            f.write(report)

        logger.info("âœ… í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")

        return report

    def save_evaluation_results(self):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        logger.info("ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥ ì¤‘...")

        # í‰ê°€ ê²°ê³¼ ìš”ì•½
        summary = {}
        for model_name, result in self.evaluation_results.items():
            summary[model_name] = {
                "test_metrics": result["test_metrics"],
                "val_metrics": result.get("val_metrics", {}),
                "model_name": model_name,
            }

        # JSON ì €ì¥
        with open(EVALUATION_DIR / "evaluation_results.json", "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        for pred_name, predictions in self.predictions.items():
            pred_df = pd.DataFrame({"predictions": predictions})
            pred_df.to_csv(
                PREDICTIONS_DIR / f"{pred_name}_predictions.csv", index=False, encoding="utf-8-sig"
            )

        logger.info("âœ… í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")


# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    # ë°ì´í„° ë¡œë” ìƒì„±
    data_loader = DataLoader()

    # ëª¨ë¸ í‰ê°€ê¸° ìƒì„±
    evaluator = ModelEvaluator(data_loader)

    # LSTM ëª¨ë¸ ë¡œë“œ (ì˜ˆì‹œ)
    lstm_model_path = LSTM_MODEL_DIR / "lstm_model.h5"
    if lstm_model_path.exists():
        evaluator.load_model("LSTM", lstm_model_path, LSTMModel)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    X_train, y_train, X_val, y_val, X_test, y_test = data_loader.prepare_lstm_dataset()

    # ëª¨ë¸ í‰ê°€
    if "LSTM" in evaluator.models:
        lstm_result = evaluator.evaluate_model("LSTM", X_test, y_test, X_val, y_val)

        # ì‹œê°í™”
        evaluator.plot_predictions("LSTM")
        evaluator.plot_residual_analysis("LSTM")

        # ë³´ê³ ì„œ ìƒì„±
        report = evaluator.generate_evaluation_report(["LSTM"])
        print(report)

        # ê²°ê³¼ ì €ì¥
        evaluator.save_evaluation_results()

    logger.info("âœ… ëª¨ë¸ í‰ê°€ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
