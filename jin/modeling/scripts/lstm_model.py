# -*- coding: utf-8 -*-
"""
LSTM ëª¨ë¸ í´ë˜ìŠ¤
Author: Jin
Created: 2025-01-15
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler
import joblib
import logging
import sys
import os
from typing import Tuple, Dict, Any, Optional
import matplotlib.pyplot as plt

# í”„ë¡œì íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from config.config import *
from scripts.utils import *
from scripts.data_loader import DataLoader

logger = logging.getLogger(__name__)


class LSTMModel:
    """LSTM ëª¨ë¸ í´ë˜ìŠ¤"""

    def __init__(self, config: Dict[str, Any] = None):
        """
        ì´ˆê¸°í™”

        Args:
            config: ëª¨ë¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config or LSTM_CONFIG
        self.model = None
        self.scaler = None
        self.history = None
        self.is_trained = False

        # TensorFlow ì„¤ì •
        tf.random.set_seed(RANDOM_SEED)

        logger.info("ğŸ¤– LSTM ëª¨ë¸ ì´ˆê¸°í™”")
        logger.info(f"ğŸ“‹ ì„¤ì •: {self.config}")

    def build_model(self) -> Model:
        """LSTM ëª¨ë¸ êµ¬ì¡° ìƒì„±"""
        logger.info("ğŸ—ï¸ LSTM ëª¨ë¸ êµ¬ì¡° ìƒì„± ì¤‘...")

        # ì…ë ¥ ë ˆì´ì–´
        inputs = Input(shape=(self.config["sequence_length"], self.config["n_features"]))

        # LSTM ë ˆì´ì–´ë“¤
        x = inputs

        # ì²« ë²ˆì§¸ LSTM ë ˆì´ì–´
        x = LSTM(
            self.config["lstm_units"][0],
            return_sequences=True,
            dropout=self.config["dropout_rate"],
            recurrent_dropout=self.config["dropout_rate"],
        )(x)
        x = BatchNormalization()(x)

        # ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´
        if len(self.config["lstm_units"]) > 1:
            x = LSTM(
                self.config["lstm_units"][1],
                return_sequences=False,
                dropout=self.config["dropout_rate"],
                recurrent_dropout=self.config["dropout_rate"],
            )(x)
            x = BatchNormalization()(x)

        # Dense ë ˆì´ì–´ë“¤
        for units in self.config["dense_units"]:
            x = Dense(units, activation="relu")(x)
            x = Dropout(self.config["dropout_rate"])(x)
            x = BatchNormalization()(x)

        # ì¶œë ¥ ë ˆì´ì–´
        outputs = Dense(1, activation="linear")(x)

        # ëª¨ë¸ ìƒì„±
        self.model = Model(inputs=inputs, outputs=outputs)

        # ëª¨ë¸ ì»´íŒŒì¼
        self.model.compile(
            optimizer=Adam(learning_rate=self.config["learning_rate"]),
            loss="mse",
            metrics=["mae", "mape"],
        )

        logger.info("âœ… LSTM ëª¨ë¸ êµ¬ì¡° ìƒì„± ì™„ë£Œ")
        self.model.summary()

        return self.model

    def prepare_data(
        self, data_loader: DataLoader
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """ë°ì´í„° ì¤€ë¹„ ë° ìŠ¤ì¼€ì¼ë§"""
        logger.info("ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # LSTM ë°ì´í„°ì…‹ ì¤€ë¹„
        X_train, y_train, X_val, y_val, X_test, y_test = data_loader.prepare_lstm_dataset(
            sequence_length=self.config["sequence_length"]
        )

        # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
        X_train_scaled, self.scaler, X_val_scaled, X_test_scaled = scale_features(
            X_train, X_val, X_test
        )

        logger.info(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
        logger.info(f"   - í•™ìŠµ ë°ì´í„°: {X_train_scaled.shape}")
        logger.info(f"   - ê²€ì¦ ë°ì´í„°: {X_val_scaled.shape}")
        logger.info(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test_scaled.shape}")

        return X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test

    def train(
        self, X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray
    ) -> Dict[str, Any]:
        """ëª¨ë¸ í•™ìŠµ"""
        logger.info("ğŸš€ LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘")

        if self.model is None:
            self.build_model()

        # ì½œë°± ì„¤ì •
        callbacks = [
            EarlyStopping(
                monitor="val_loss",
                patience=self.config["patience"],
                restore_best_weights=True,
                verbose=1,
            ),
            ModelCheckpoint(
                filepath=str(LSTM_MODEL_DIR / "best_model.h5"),
                monitor="val_loss",
                save_best_only=True,
                save_weights_only=False,
                verbose=1,
            ),
            ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=5, min_lr=1e-7, verbose=1),
        ]

        # ëª¨ë¸ í•™ìŠµ
        self.history = self.model.fit(
            X_train,
            y_train,
            validation_data=(X_val, y_val),
            epochs=self.config["epochs"],
            batch_size=self.config["batch_size"],
            callbacks=callbacks,
            verbose=1,
        )

        self.is_trained = True
        logger.info("âœ… LSTM ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")

        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        self.save_training_history()

        return self.history.history

    def predict(self, X: np.ndarray) -> np.ndarray:
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        if not self.is_trained or self.model is None:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € train()ì„ ì‹¤í–‰í•˜ì„¸ìš”.")

        logger.info("ğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
        predictions = self.model.predict(X, verbose=0)

        return predictions.flatten()

    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€"""
        if not self.is_trained or self.model is None:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € train()ì„ ì‹¤í–‰í•˜ì„¸ìš”.")

        logger.info("ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")

        # ì˜ˆì¸¡ ìˆ˜í–‰
        y_pred = self.predict(X)

        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        metrics = calculate_metrics(y, y_pred)

        # ê²°ê³¼ ì¶œë ¥
        print_metrics(metrics, "LSTM ëª¨ë¸ í‰ê°€ ê²°ê³¼")

        return metrics

    def save_model(self, filepath: str = None):
        """ëª¨ë¸ ì €ì¥"""
        if self.model is None:
            logger.warning("ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        if filepath is None:
            filepath = LSTM_MODEL_DIR / "lstm_model.h5"

        # ëª¨ë¸ ì €ì¥
        self.model.save(filepath)

        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        if self.scaler is not None:
            scaler_path = str(filepath).replace(".h5", "_scaler.pkl")
            joblib.dump(self.scaler, scaler_path)

        # ì„¤ì • ì €ì¥
        config_path = str(filepath).replace(".h5", "_config.pkl")
        joblib.dump(self.config, config_path)

        logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")

    def load_model(self, filepath: str = None):
        """ëª¨ë¸ ë¡œë“œ"""
        if filepath is None:
            filepath = LSTM_MODEL_DIR / "lstm_model.h5"

        try:
            # ëª¨ë¸ ë¡œë“œ
            self.model = tf.keras.models.load_model(filepath)

            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            scaler_path = str(filepath).replace(".h5", "_scaler.pkl")
            if os.path.exists(scaler_path):
                self.scaler = joblib.load(scaler_path)

            # ì„¤ì • ë¡œë“œ
            config_path = str(filepath).replace(".h5", "_config.pkl")
            if os.path.exists(config_path):
                self.config = joblib.load(config_path)

            self.is_trained = True
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise

    def save_training_history(self):
        """í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥ ë° ì‹œê°í™”"""
        if self.history is None:
            return

        history_df = pd.DataFrame(self.history.history)
        history_df.to_csv(RESULTS_DIR / "lstm_training_history.csv", index=False)

        # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
        self.plot_training_history()

        logger.info("âœ… í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ")

    def plot_training_history(self):
        """í•™ìŠµ ê³¡ì„  ì‹œê°í™”"""
        if self.history is None:
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # Loss ê³¡ì„ 
        axes[0, 0].plot(self.history.history["loss"], label="Train Loss")
        axes[0, 0].plot(self.history.history["val_loss"], label="Val Loss")
        axes[0, 0].set_title("Model Loss")
        axes[0, 0].set_xlabel("Epoch")
        axes[0, 0].set_ylabel("Loss")
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # MAE ê³¡ì„ 
        axes[0, 1].plot(self.history.history["mae"], label="Train MAE")
        axes[0, 1].plot(self.history.history["val_mae"], label="Val MAE")
        axes[0, 1].set_title("Model MAE")
        axes[0, 1].set_xlabel("Epoch")
        axes[0, 1].set_ylabel("MAE")
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # MAPE ê³¡ì„ 
        axes[1, 0].plot(self.history.history["mape"], label="Train MAPE")
        axes[1, 0].plot(self.history.history["val_mape"], label="Val MAPE")
        axes[1, 0].set_title("Model MAPE")
        axes[1, 0].set_xlabel("Epoch")
        axes[1, 0].set_ylabel("MAPE")
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # í•™ìŠµë¥  ê³¡ì„  (ìˆì„ ê²½ìš°)
        if "lr" in self.history.history:
            axes[1, 1].plot(self.history.history["lr"], label="Learning Rate")
            axes[1, 1].set_title("Learning Rate")
            axes[1, 1].set_xlabel("Epoch")
            axes[1, 1].set_ylabel("Learning Rate")
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        else:
            axes[1, 1].text(
                0.5,
                0.5,
                "No LR data",
                ha="center",
                va="center",
                transform=axes[1, 1].transAxes,
                fontsize=12,
            )
            axes[1, 1].set_title("Learning Rate")

        plt.tight_layout()
        plt.savefig(PLOTS_DIR / "lstm_training_history.png", dpi=300, bbox_inches="tight")
        plt.show()

    def predict_future(self, data_loader: DataLoader, n_periods: int = 6) -> np.ndarray:
        """ë¯¸ë˜ ì˜ˆì¸¡"""
        if not self.is_trained or self.model is None:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € train()ì„ ì‹¤í–‰í•˜ì„¸ìš”.")

        logger.info(f"ğŸ”® í–¥í›„ {n_periods}ê°œì›” ì˜ˆì¸¡ ì¤‘...")

        # ìµœê·¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        data = data_loader.processed_data

        # êµ­ì -ëª©ì ë³„ ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ìƒì„±
        future_predictions = []

        for (country, purpose), group in data.groupby(["êµ­ì ", "ëª©ì "]):
            group_sorted = group.sort_values(["ì—°ë„", "ì›”"])

            if len(group_sorted) >= self.config["sequence_length"]:
                # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì¶”ì¶œ
                last_sequence = (
                    group_sorted[FEATURE_COLUMNS].tail(self.config["sequence_length"]).values
                )

                # ìŠ¤ì¼€ì¼ë§
                if self.scaler is not None:
                    last_sequence = self.scaler.transform(
                        last_sequence.reshape(-1, last_sequence.shape[-1])
                    )
                    last_sequence = last_sequence.reshape(1, self.config["sequence_length"], -1)

                # ì˜ˆì¸¡
                pred = self.model.predict(last_sequence, verbose=0)[0, 0]

                future_predictions.append({"êµ­ì ": country, "ëª©ì ": purpose, "ì˜ˆì¸¡ê°’": pred})

        predictions_df = pd.DataFrame(future_predictions)

        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        predictions_df.to_csv(
            PREDICTIONS_DIR / "lstm_future_predictions.csv", index=False, encoding="utf-8-sig"
        )

        logger.info(f"âœ… ë¯¸ë˜ ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions_df)}ê°œ ì˜ˆì¸¡")

        return predictions_df

    def get_model_summary(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìš”ì•½ ì •ë³´"""
        if self.model is None:
            return {"error": "ëª¨ë¸ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        summary = {
            "model_type": "LSTM",
            "total_params": self.model.count_params(),
            "trainable_params": sum(
                [tf.keras.backend.count_params(w) for w in self.model.trainable_weights]
            ),
            "config": self.config,
            "is_trained": self.is_trained,
        }

        if self.history is not None:
            summary["best_val_loss"] = min(self.history.history["val_loss"])
            summary["training_epochs"] = len(self.history.history["loss"])

        return summary


# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    # ë°ì´í„° ë¡œë” ìƒì„±
    data_loader = DataLoader()

    # LSTM ëª¨ë¸ ìƒì„±
    lstm_model = LSTMModel()

    # ëª¨ë¸ êµ¬ì¡° ìƒì„±
    lstm_model.build_model()

    # ë°ì´í„° ì¤€ë¹„
    X_train, y_train, X_val, y_val, X_test, y_test = lstm_model.prepare_data(data_loader)

    # ëª¨ë¸ í•™ìŠµ
    history = lstm_model.train(X_train, y_train, X_val, y_val)

    # ëª¨ë¸ í‰ê°€
    test_metrics = lstm_model.evaluate(X_test, y_test)

    # ëª¨ë¸ ì €ì¥
    lstm_model.save_model()

    # ë¯¸ë˜ ì˜ˆì¸¡
    future_predictions = lstm_model.predict_future(data_loader)

    # ëª¨ë¸ ìš”ì•½
    summary = lstm_model.get_model_summary()
    print("\nğŸ“‹ ëª¨ë¸ ìš”ì•½:")
    for key, value in summary.items():
        print(f"  {key}: {value}")

    logger.info("âœ… LSTM ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
