# -*- coding: utf-8 -*-
"""
ëª¨ë¸ í•™ìŠµ ê´€ë¦¬ì
Author: Jin
Created: 2025-01-15
"""

import pandas as pd
import numpy as np
import logging
import sys
import os
import argparse
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import warnings

# í”„ë¡œì íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from config.config import *
from scripts.utils import *
from scripts.data_loader import DataLoader
from scripts.lstm_model import LSTMModel

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)


class ModelTrainer:
    """ëª¨ë¸ í•™ìŠµ ê´€ë¦¬ì í´ë˜ìŠ¤"""

    def __init__(self, data_loader: DataLoader = None):
        """
        ì´ˆê¸°í™”

        Args:
            data_loader: ë°ì´í„° ë¡œë” ì¸ìŠ¤í„´ìŠ¤
        """
        self.data_loader = data_loader or DataLoader()
        self.models = {}
        self.training_results = {}
        self.best_model = None
        self.best_model_name = None
        self.best_score = float("inf")

        # í•™ìŠµ ì‹œì‘ ì‹œê°„
        self.start_time = None

        logger.info("ğŸ¯ ëª¨ë¸ í•™ìŠµ ê´€ë¦¬ì ì´ˆê¸°í™”")

    def register_model(self, name: str, model_class, config: Dict[str, Any] = None):
        """
        ëª¨ë¸ ë“±ë¡

        Args:
            name: ëª¨ë¸ ì´ë¦„
            model_class: ëª¨ë¸ í´ë˜ìŠ¤
            config: ëª¨ë¸ ì„¤ì •
        """
        self.models[name] = {
            "class": model_class,
            "config": config,
            "instance": None,
            "trained": False,
            "metrics": None,
        }

        logger.info(f"ğŸ“ ëª¨ë¸ ë“±ë¡: {name}")

    def train_single_model(
        self,
        model_name: str,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        X_test: np.ndarray,
        y_test: np.ndarray,
    ) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ

        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            X_train, y_train: í•™ìŠµ ë°ì´í„°
            X_val, y_val: ê²€ì¦ ë°ì´í„°
            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°

        Returns:
            í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if model_name not in self.models:
            raise ValueError(f"ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")

        logger.info(f"ğŸš€ {model_name} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        start_time = time.time()

        model_info = self.models[model_name]

        try:
            # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            if model_info["config"]:
                model_instance = model_info["class"](model_info["config"])
            else:
                model_instance = model_info["class"]()

            # ëª¨ë¸ í•™ìŠµ
            if hasattr(model_instance, "prepare_data"):
                # ë°ì´í„° ì¤€ë¹„ê°€ í•„ìš”í•œ ê²½ìš° (ì˜ˆ: LSTM)
                X_train_prep, y_train_prep, X_val_prep, y_val_prep, X_test_prep, y_test_prep = (
                    model_instance.prepare_data(self.data_loader)
                )

                # í•™ìŠµ ì‹¤í–‰
                training_history = model_instance.train(
                    X_train_prep, y_train_prep, X_val_prep, y_val_prep
                )

                # í‰ê°€ ì‹¤í–‰
                test_metrics = model_instance.evaluate(X_test_prep, y_test_prep)
                val_metrics = model_instance.evaluate(X_val_prep, y_val_prep)

            else:
                # ì¼ë°˜ì ì¸ ê²½ìš°
                training_history = model_instance.train(X_train, y_train, X_val, y_val)
                test_metrics = model_instance.evaluate(X_test, y_test)
                val_metrics = model_instance.evaluate(X_val, y_val)

            # í•™ìŠµ ì‹œê°„ ê³„ì‚°
            training_time = time.time() - start_time

            # ê²°ê³¼ ì €ì¥
            result = {
                "model_name": model_name,
                "training_time": training_time,
                "training_history": training_history,
                "test_metrics": test_metrics,
                "val_metrics": val_metrics,
                "model_instance": model_instance,
                "success": True,
                "error": None,
            }

            # ëª¨ë¸ ì •ë³´ ì—…ë°ì´íŠ¸
            self.models[model_name]["instance"] = model_instance
            self.models[model_name]["trained"] = True
            self.models[model_name]["metrics"] = test_metrics

            # ìµœì  ëª¨ë¸ ì—…ë°ì´íŠ¸
            current_score = test_metrics.get(MODEL_SELECTION_METRIC, float("inf"))
            if current_score < self.best_score:
                self.best_score = current_score
                self.best_model = model_instance
                self.best_model_name = model_name

                logger.info(
                    f"ğŸ† ìƒˆë¡œìš´ ìµœì  ëª¨ë¸: {model_name} ({MODEL_SELECTION_METRIC}: {current_score:.4f})"
                )

            logger.info(f"âœ… {model_name} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ({training_time:.2f}ì´ˆ)")

            return result

        except Exception as e:
            logger.error(f"âŒ {model_name} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {str(e)}")

            result = {
                "model_name": model_name,
                "training_time": time.time() - start_time,
                "training_history": None,
                "test_metrics": None,
                "val_metrics": None,
                "model_instance": None,
                "success": False,
                "error": str(e),
            }

            return result

    def train_all_models(self, save_models: bool = True) -> Dict[str, Any]:
        """
        ëª¨ë“  ë“±ë¡ëœ ëª¨ë¸ í•™ìŠµ

        Args:
            save_models: ëª¨ë¸ ì €ì¥ ì—¬ë¶€

        Returns:
            ì „ì²´ í•™ìŠµ ê²°ê³¼
        """
        logger.info("ğŸ¯ ì „ì²´ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        self.start_time = time.time()

        # ë°ì´í„° ì¤€ë¹„
        logger.info("ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        X_train, y_train, X_val, y_val, X_test, y_test = self.data_loader.prepare_lstm_dataset()

        # ê° ëª¨ë¸ í•™ìŠµ
        for model_name in self.models.keys():
            logger.info(f"\n{'='*50}")
            logger.info(f"ğŸ¯ {model_name} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
            logger.info(f"{'='*50}")

            result = self.train_single_model(
                model_name, X_train, y_train, X_val, y_val, X_test, y_test
            )

            self.training_results[model_name] = result

            # ëª¨ë¸ ì €ì¥
            if save_models and result["success"] and result["model_instance"]:
                try:
                    model_instance = result["model_instance"]
                    if hasattr(model_instance, "save_model"):
                        save_path = MODEL_DIR / model_name.lower() / f"{model_name.lower()}_model"
                        model_instance.save_model(save_path)
                        logger.info(f"ğŸ’¾ {model_name} ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

        # ì „ì²´ í•™ìŠµ ì‹œê°„
        total_time = time.time() - self.start_time

        logger.info(f"\n{'='*50}")
        logger.info(f"âœ… ì „ì²´ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ({total_time:.2f}ì´ˆ)")
        logger.info(
            f"ğŸ† ìµœì  ëª¨ë¸: {self.best_model_name} ({MODEL_SELECTION_METRIC}: {self.best_score:.4f})"
        )
        logger.info(f"{'='*50}")

        # ê²°ê³¼ ì €ì¥
        self.save_training_results()

        return self.training_results

    def save_training_results(self):
        """í•™ìŠµ ê²°ê³¼ ì €ì¥"""
        logger.info("ğŸ’¾ í•™ìŠµ ê²°ê³¼ ì €ì¥ ì¤‘...")

        # ê²°ê³¼ ìš”ì•½ ìƒì„±
        summary = self.generate_training_summary()

        # JSON ì €ì¥
        results_for_json = {}
        for model_name, result in self.training_results.items():
            results_for_json[model_name] = {
                "model_name": result["model_name"],
                "training_time": result["training_time"],
                "test_metrics": result["test_metrics"],
                "val_metrics": result["val_metrics"],
                "success": result["success"],
                "error": result["error"],
            }

        # ì „ì²´ ê²°ê³¼ ì €ì¥
        final_results = {
            "training_summary": summary,
            "model_results": results_for_json,
            "best_model": self.best_model_name,
            "best_score": self.best_score,
            "training_date": datetime.now().isoformat(),
        }

        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(RESULTS_DIR / "training_results.json", "w", encoding="utf-8") as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False)

        # CSV íŒŒì¼ë¡œ ì €ì¥
        summary_df = pd.DataFrame(summary).T
        summary_df.to_csv(RESULTS_DIR / "training_summary.csv", encoding="utf-8-sig")

        logger.info("âœ… í•™ìŠµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

    def generate_training_summary(self) -> Dict[str, Dict[str, Any]]:
        """í•™ìŠµ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        summary = {}

        for model_name, result in self.training_results.items():
            if result["success"]:
                summary[model_name] = {
                    "training_time": result["training_time"],
                    "test_mae": result["test_metrics"]["mae"],
                    "test_mse": result["test_metrics"]["mse"],
                    "test_rmse": result["test_metrics"]["rmse"],
                    "test_mape": result["test_metrics"]["mape"],
                    "test_r2": result["test_metrics"]["r2"],
                    "val_mae": result["val_metrics"]["mae"],
                    "val_mse": result["val_metrics"]["mse"],
                    "val_rmse": result["val_metrics"]["rmse"],
                    "val_mape": result["val_metrics"]["mape"],
                    "val_r2": result["val_metrics"]["r2"],
                    "status": "success",
                }
            else:
                summary[model_name] = {
                    "training_time": result["training_time"],
                    "test_mae": None,
                    "test_mse": None,
                    "test_rmse": None,
                    "test_mape": None,
                    "test_r2": None,
                    "val_mae": None,
                    "val_mse": None,
                    "val_rmse": None,
                    "val_mape": None,
                    "val_r2": None,
                    "status": "failed",
                    "error": result["error"],
                }

        return summary

    def compare_models(self) -> pd.DataFrame:
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        logger.info("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì¤‘...")

        comparison_data = []

        for model_name, result in self.training_results.items():
            if result["success"]:
                comparison_data.append(
                    {
                        "Model": model_name,
                        "Training Time (s)": result["training_time"],
                        "Test MAE": result["test_metrics"]["mae"],
                        "Test RMSE": result["test_metrics"]["rmse"],
                        "Test MAPE": result["test_metrics"]["mape"],
                        "Test RÂ²": result["test_metrics"]["r2"],
                        "Val MAE": result["val_metrics"]["mae"],
                        "Val RMSE": result["val_metrics"]["rmse"],
                        "Val MAPE": result["val_metrics"]["mape"],
                        "Val RÂ²": result["val_metrics"]["r2"],
                        "Status": "Success",
                    }
                )
            else:
                comparison_data.append(
                    {
                        "Model": model_name,
                        "Training Time (s)": result["training_time"],
                        "Test MAE": None,
                        "Test RMSE": None,
                        "Test MAPE": None,
                        "Test RÂ²": None,
                        "Val MAE": None,
                        "Val RMSE": None,
                        "Val MAPE": None,
                        "Val RÂ²": None,
                        "Status": "Failed",
                    }
                )

        comparison_df = pd.DataFrame(comparison_data)

        # ê²°ê³¼ ì €ì¥
        comparison_df.to_csv(
            RESULTS_DIR / "model_comparison.csv", index=False, encoding="utf-8-sig"
        )

        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
        print("=" * 80)
        print(comparison_df.to_string(index=False))
        print("=" * 80)

        return comparison_df

    def get_best_model(self) -> Tuple[str, Any]:
        """ìµœì  ëª¨ë¸ ë°˜í™˜"""
        if self.best_model is None:
            logger.warning("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None, None

        return self.best_model_name, self.best_model

    def save_best_model(self):
        """ìµœì  ëª¨ë¸ ì €ì¥"""
        if self.best_model is None:
            logger.warning("ì €ì¥í•  ìµœì  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ìµœì  ëª¨ë¸ ì €ì¥
        if hasattr(self.best_model, "save_model"):
            save_path = BEST_MODEL_DIR / f"best_{self.best_model_name.lower()}_model"
            self.best_model.save_model(save_path)

            # ìµœì  ëª¨ë¸ ì •ë³´ ì €ì¥
            best_model_info = {
                "model_name": self.best_model_name,
                "score": self.best_score,
                "metric": MODEL_SELECTION_METRIC,
                "metrics": self.training_results[self.best_model_name]["test_metrics"],
                "save_path": str(save_path),
                "save_date": datetime.now().isoformat(),
            }

            with open(BEST_MODEL_DIR / "best_model_info.json", "w", encoding="utf-8") as f:
                json.dump(best_model_info, f, indent=2, ensure_ascii=False)

            logger.info(f"ğŸ† ìµœì  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.best_model_name}")

    def generate_report(self) -> str:
        """í•™ìŠµ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
        logger.info("ğŸ“‹ í•™ìŠµ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„± ì¤‘...")

        total_time = time.time() - self.start_time if self.start_time else 0

        report = f"""
# ëª¨ë¸ í•™ìŠµ ê²°ê³¼ ë³´ê³ ì„œ

## ğŸ“Š í•™ìŠµ ê°œìš”
- í•™ìŠµ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- ì´ í•™ìŠµ ì‹œê°„: {total_time:.2f}ì´ˆ
- í•™ìŠµ ëª¨ë¸ ìˆ˜: {len(self.models)}
- ì„±ê³µ ëª¨ë¸ ìˆ˜: {sum(1 for r in self.training_results.values() if r['success'])}

## ğŸ† ìµœì  ëª¨ë¸
- ëª¨ë¸ëª…: {self.best_model_name}
- í‰ê°€ ì§€í‘œ: {MODEL_SELECTION_METRIC}
- ì ìˆ˜: {self.best_score:.4f}

## ğŸ“ˆ ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
"""

        for model_name, result in self.training_results.items():
            if result["success"]:
                report += f"""
### {model_name}
- í•™ìŠµ ì‹œê°„: {result['training_time']:.2f}ì´ˆ
- Test MAE: {result['test_metrics']['mae']:.4f}
- Test RMSE: {result['test_metrics']['rmse']:.4f}
- Test MAPE: {result['test_metrics']['mape']:.4f}%
- Test RÂ²: {result['test_metrics']['r2']:.4f}
"""
            else:
                report += f"""
### {model_name}
- ìƒíƒœ: í•™ìŠµ ì‹¤íŒ¨
- ì˜¤ë¥˜: {result['error']}
"""

        report += f"""
## ğŸ¯ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
1. ìµœì  ëª¨ë¸: {self.best_model_name}
2. ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ: {MODEL_SELECTION_METRIC} = {self.best_score:.4f}
3. ë‹¤ìŒ ë‹¨ê³„: ìµœì  ëª¨ë¸ì„ ì‚¬ìš©í•œ ì˜ˆì¸¡ ìˆ˜í–‰

---
ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

        # ë³´ê³ ì„œ ì €ì¥
        with open(RESULTS_DIR / "training_report.md", "w", encoding="utf-8") as f:
            f.write(report)

        logger.info("âœ… í•™ìŠµ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")

        return report


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ëª¨ë¸ í•™ìŠµ ê´€ë¦¬ì")
    parser.add_argument(
        "--models", nargs="+", default=["lstm"], help="í•™ìŠµí•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (lstm, prophet, etc.)"
    )
    parser.add_argument("--save-models", action="store_true", default=True, help="ëª¨ë¸ ì €ì¥ ì—¬ë¶€")
    parser.add_argument("--compare", action="store_true", default=True, help="ëª¨ë¸ ë¹„êµ ìˆ˜í–‰ ì—¬ë¶€")

    args = parser.parse_args()

    # ë°ì´í„° ë¡œë” ìƒì„±
    data_loader = DataLoader()

    # ëª¨ë¸ í•™ìŠµ ê´€ë¦¬ì ìƒì„±
    trainer = ModelTrainer(data_loader)

    # ëª¨ë¸ ë“±ë¡
    if "lstm" in args.models:
        trainer.register_model("LSTM", LSTMModel, LSTM_CONFIG)

    # ì¶”ê°€ ëª¨ë¸ë“¤ì€ ì—¬ê¸°ì— ë“±ë¡
    # if 'prophet' in args.models:
    #     trainer.register_model('Prophet', ProphetModel, PROPHET_CONFIG)

    # ì „ì²´ ëª¨ë¸ í•™ìŠµ
    results = trainer.train_all_models(save_models=args.save_models)

    # ëª¨ë¸ ë¹„êµ
    if args.compare:
        comparison_df = trainer.compare_models()

    # ìµœì  ëª¨ë¸ ì €ì¥
    trainer.save_best_model()

    # ë³´ê³ ì„œ ìƒì„±
    report = trainer.generate_report()
    print(report)

    logger.info("ğŸ‰ ëª¨ë¸ í•™ìŠµ ê´€ë¦¬ì ì‹¤í–‰ ì™„ë£Œ")


if __name__ == "__main__":
    main()
